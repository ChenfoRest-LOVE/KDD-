根据知识库信息和比赛要求，我为你制定了详细的任务实施计划。以下是结合Meta CRAG比赛要求的细化任务分解：

# Meta CRAG 复现项目详细实施计划

## 项目目标
复现**USTC (APEX) 第二名方案**的核心组件：**Domain Router + Dynamic Router + Retriever + Reader**，并在此基础上进行改进实验，最终形成可投稿的技术方案。

---

## 阶段一：数据准备与清洗（第1周）

### 1.1 数据获取
```bash
# 下载官方赛题数据
wget <aicrowd-dataset-url>  # 从比赛官网获取
unzip crag_dataset.zip
```

### 1.2 数据格式分析
**原始数据结构**（Meta CRAG官方格式）：
```json
{
    "interaction_id": "sample_001",
    "query": "What is the current stock price of Apple?",
    "search_results": [
        {"title": "Apple Inc. Stock", "snippet": "AAPL is trading at...", "url": "..."},
        {"title": "Tech Stocks Today", "snippet": "Apple stock rose...", "url": "..."}
    ],
    "query_time": "2024-01-15T14:30:00Z",
    "answer": "Apple's current stock price is $185.23"
}
```

### 1.3 数据清洗脚本实现
创建 `src/preprocessing.py`：

```python
import json
import bz2
from datetime import datetime
import re
from tqdm import tqdm

def clean_html_content(text):
    """清理HTML标签和特殊字符"""
    # 移除HTML标签
    text = re.sub(r'<[^>]+>', ' ', text)
    # 标准化空白字符
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def chunk_text(text, chunk_size=512, overlap=50):
    """将长文本切分为chunk"""
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        if len(chunk.strip()) > 20:  # 过滤过短chunk
            chunks.append(chunk)
    return chunks

def preprocess_crag_data(input_path, output_path):
    """
    将原始CRAG数据转换为目标格式
    """
    processed_data = []
    
    with open(input_path, 'r', encoding='utf-8') as file:
        for line in tqdm(file, desc="Processing data"):
            try:
                item = json.loads(line)
                
                # 提取并清洗search_results
                cleaned_search_results = []
                for result in item.get('search_results', []):
                    # 合并title和snippet
                    content = f"{result.get('title', '')} {result.get('snippet', '')}"
                    cleaned_content = clean_html_content(content)
                    
                    # 将长内容chunk化
                    chunks = chunk_text(cleaned_content, chunk_size=512)
                    cleaned_search_results.extend(chunks)
                
                # 构造目标格式
                processed_item = {
                    "interaction_id": item["interaction_id"],
                    "query": item["query"],
                    "search_results": cleaned_search_results[:10],  # 限制chunk数量
                    "query_time": item["query_time"],
                    "answer": item["answer"]
                }
                
                processed_data.append(processed_item)
                
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Error processing line: {e}")
                continue
    
    # 保存为压缩JSONL格式
    with bz2.open(output_path, 'wt', encoding='utf-8') as file:
        for item in processed_data:
            file.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"Processed {len(processed_data)} samples to {output_path}")

# 使用示例
if __name__ == "__main__":
    preprocess_crag_data(
        input_path="data/raw/crag_dev.jsonl",
        output_path="data/cleaned/dev_data.jsonl.bz2"
    )
```

---

## 阶段二：路由器模型实现（第2-3周）

### 2.1 Domain Router实现

**目标**：将问题分类到`["finance", "music", "movie", "sports", "open"]`五个领域

#### 2.1.1 训练数据生成
创建 `src/routers/generate_domain_labels.py`：

```python
import json
import openai
from tqdm import tqdm

def generate_domain_labels_with_llm(queries, api_key):
    """使用GPT-4生成领域标签"""
    openai.api_key = api_key
    
    domain_prompt = """
    请将以下问题分类到这五个领域之一：finance, music, movie, sports, open
    
    规则：
    - finance: 股票、投资、经济、公司财务相关
    - music: 歌曲、歌手、专辑、音乐奖项相关  
    - movie: 电影、演员、导演、电影奖项相关
    - sports: 体育比赛、运动员、体育赛事相关
    - open: 其他通用知识问题
    
    问题: {query}
    领域: 
    """
    
    labeled_data = []
    for query in tqdm(queries, desc="Generating domain labels"):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{
                    "role": "user", 
                    "content": domain_prompt.format(query=query)
                }],
                temperature=0,
                max_tokens=10
            )
            
            domain = response.choices[0].message.content.strip().lower()
            if domain in ["finance", "music", "movie", "sports", "open"]:
                labeled_data.append({"query": query, "domain": domain})
        
        except Exception as e:
            print(f"Error processing query: {e}")
            labeled_data.append({"query": query, "domain": "open"})  # 默认标签
    
    return labeled_data
```

#### 2.1.2 Domain Router训练
创建 `src/routers/train_domain_router.py`：

```python
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification, 
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset
import torch

class DomainRouterTrainer:
    def __init__(self, model_name="microsoft/deberta-v3-base"):
        self.model_name = model_name
        self.classes = ["finance", "music", "movie", "sports", "open"]
        self.label2id = {label: i for i, label in enumerate(self.classes)}
        self.id2label = {i: label for label, i in self.label2id.items()}
        
    def prepare_data(self, labeled_data):
        """准备训练数据"""
        queries = [item["query"] for item in labeled_data]
        labels = [self.label2id[item["domain"]] for item in labeled_data]
        
        # 80-20分割训练/验证集
        split_idx = int(0.8 * len(queries))
        train_data = Dataset.from_dict({
            "text": queries[:split_idx],
            "labels": labels[:split_idx]
        })
        eval_data = Dataset.from_dict({
            "text": queries[split_idx:],
            "labels": labels[split_idx:]
        })
        
        return train_data, eval_data
    
    def tokenize_function(self, examples):
        return self.tokenizer(
            examples["text"], 
            truncation=True, 
            padding=True, 
            max_length=512
        )
    
    def train(self, labeled_data, output_dir="models/router/domain"):
        """训练Domain Router"""
        # 初始化tokenizer和model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.classes),
            label2id=self.label2id,
            id2label=self.id2label
        )
        
        # 准备数据
        train_dataset, eval_dataset = self.prepare_data(labeled_data)
        train_dataset = train_dataset.map(self.tokenize_function, batched=True)
        eval_dataset = eval_dataset.map(self.tokenize_function, batched=True)
        
        # 训练配置
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            learning_rate=2e-5,
            warmup_steps=500,
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500,
            save_strategy="steps",
            save_steps=500,
            load_best_model_at_end=True,
        )
        
        # 初始化Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=DataCollatorWithPadding(self.tokenizer),
        )
        
        # 开始训练
        trainer.train()
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        return trainer
```

### 2.2 Dynamic Router实现

**目标**：将问题分类到`["static", "slow-changing", "fast-changing", "real-time"]`四个动态性级别

#### 2.2.1 规则+LLM混合标注
创建 `src/routers/generate_dynamic_labels.py`：

```python
import re
from datetime import datetime, timedelta

def rule_based_dynamic_detection(query, query_time=None):
    """基于规则的动态性检测"""
    query_lower = query.lower()
    
    # 实时关键词
    real_time_keywords = [
        'current', 'now', 'today', 'latest', 'recent', 'live', 
        'breaking', 'just', 'this morning', 'tonight'
    ]
    
    # 快变关键词  
    fast_changing_keywords = [
        'stock price', 'score', 'weather', 'news', 'trending',
        'this week', 'this month', 'yesterday'
    ]
    
    # 慢变关键词
    slow_changing_keywords = [
        'population', 'gdp', 'annual', 'yearly', 'census'
    ]
    
    # 静态关键词
    static_keywords = [
        'born', 'died', 'founded', 'invented', 'discovered',
        'history', 'biography', 'definition'
    ]
    
    # 规则匹配
    if any(keyword in query_lower for keyword in real_time_keywords):
        return "real-time"
    elif any(keyword in query_lower for keyword in fast_changing_keywords):
        return "fast-changing"  
    elif any(keyword in query_lower for keyword in slow_changing_keywords):
        return "slow-changing"
    elif any(keyword in query_lower for keyword in static_keywords):
        return "static"
    else:
        return None  # 需要LLM判断

def generate_dynamic_labels_hybrid(queries, query_times, api_key):
    """规则+LLM混合标注"""
    dynamic_prompt = """
    请将以下问题按信息变化频率分类：
    - static: 历史事实、人物传记、定义等不变信息
    - slow-changing: 人口、GDP等年度变化的信息  
    - fast-changing: 股价、天气、新闻等每日变化的信息
    - real-time: 比赛比分、突发新闻等实时变化的信息
    
    问题: {query}
    类别:
    """
    
    labeled_data = []
    
    for query, query_time in zip(queries, query_times):
        # 先尝试规则检测
        rule_result = rule_based_dynamic_detection(query, query_time)
        
        if rule_result:
            dynamic_level = rule_result
        else:
            # 使用LLM判断
            try:
                response = openai.ChatCompletion.create(
                    model="gpt-4",
                    messages=[{
                        "role": "user",
                        "content": dynamic_prompt.format(query=query)
                    }],
                    temperature=0,
                    max_tokens=15
                )
                
                dynamic_level = response.choices[0].message.content.strip().lower()
                if dynamic_level not in ["static", "slow-changing", "fast-changing", "real-time"]:
                    dynamic_level = "static"  # 默认值
                    
            except Exception as e:
                print(f"Error with LLM: {e}")
                dynamic_level = "static"
        
        labeled_data.append({
            "query": query,
            "query_time": query_time, 
            "dynamic_level": dynamic_level
        })
    
    return labeled_data
```

---

## 阶段三：检索与索引构建（第3周）

### 3.1 构建向量索引
创建 `src/indexer.py`：

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import json
import pickle
from tqdm import tqdm

class DocumentIndexer:
    def __init__(self, embedding_model="BAAI/bge-m3"):
        self.embedding_model = SentenceTransformer(embedding_model)
        self.documents = []
        self.embeddings = None
        self.index = None
        
    def add_documents(self, cleaned_data_path):
        """添加文档到索引"""
        print("Loading and embedding documents...")
        
        with open(cleaned_data_path, 'r') as file:
            for line in tqdm(file, desc="Processing documents"):
                item = json.loads(line)
                
                # 为每个search_result创建文档
                for i, chunk in enumerate(item["search_results"]):
                    doc = {
                        "doc_id": f"{item['interaction_id']}_chunk_{i}",
                        "content": chunk,
                        "source_query": item["query"],
                        "query_time": item["query_time"]
                    }
                    self.documents.append(doc)
        
        # 生成embeddings
        contents = [doc["content"] for doc in self.documents]
        self.embeddings = self.embedding_model.encode(
            contents, 
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
    def build_faiss_index(self, index_type="IVF"):
        """构建FAISS索引"""
        embedding_dim = self.embeddings.shape[1]
        
        if index_type == "IVF":
            # IVF索引（适合大规模数据）
            nlist = min(4096, len(self.documents) // 39)  # 聚类中心数
            quantizer = faiss.IndexFlatIP(embedding_dim)  # 内积距离
            self.index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)
            
            # 训练索引
            self.index.train(self.embeddings.astype(np.float32))
            self.index.add(self.embeddings.astype(np.float32))
            self.index.nprobe = min(64, nlist)  # 搜索的聚类数
            
        else:
            # 简单的平面索引
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.index.add(self.embeddings.astype(np.float32))
    
    def save_index(self, output_dir):
        """保存索引和文档"""
        # 保存FAISS索引
        faiss.write_index(self.index, f"{output_dir}/faiss.index")
        
        # 保存文档映射
        with open(f"{output_dir}/documents.pkl", 'wb') as f:
            pickle.dump(self.documents, f)
            
        # 保存embedding模型配置
        with open(f"{output_dir}/config.json", 'w') as f:
            json.dump({
                "embedding_model": self.embedding_model.get_sentence_embedding_dimension(),
                "num_documents": len(self.documents),
                "embedding_dim": self.embeddings.shape[1]
            }, f)
        
        print(f"Index saved to {output_dir}")

# 使用示例
if __name__ == "__main__":
    indexer = DocumentIndexer()
    indexer.add_documents("data/cleaned/dev_data.jsonl")
    indexer.build_faiss_index()
    indexer.save_index("index/")
```

---

## 阶段四：端到端Pipeline集成（第4周）

### 4.1 更新RAGModel实现
基于之前的代码，创建完整的 `src/models/model.py`：

```python
class RAGModel:
    def __init__(self, chat_model, retriever, domain_router, dynamic_router=None, use_kg=True):
        self.chat_model = chat_model
        self.retriever = retriever  
        self.domain_router = domain_router
        self.dynamic_router = dynamic_router
        self.use_kg = use_kg
        
    def route_query(self, query, query_time=None):
        """查询路由处理"""
        # Domain routing
        domain = self.domain_router.predict(query)
        
        # Dynamic routing  
        if self.dynamic_router:
            dynamic_level = self.dynamic_router.predict(query)
            
            # 根据动态性决定是否调用实时API
            if dynamic_level in ["fast-changing", "real-time"]:
                # 调用mock API或知识图谱
                api_results = self.call_mock_api(query) if self.use_kg else []
                return domain, dynamic_level, api_results
        
        return domain, "static", []
    
    def call_mock_api(self, query):
        """调用模拟API获取实时信息"""
        import requests
        import os
        
        try:
            mock_api_url = os.environ.get("CRAG_MOCK_API_URL", "http://localhost:8000")
            response = requests.post(
                f"{mock_api_url}/query",
                json={"query": query},
                timeout=5
            )
            return response.json().get("results", [])
        except Exception as e:
            print(f"Mock API error: {e}")
            return []
    
    def generate_answer(self, query, search_results, query_time=None, api_results=None):
        """生成最终答案"""
        # 路由处理
        domain, dynamic_level, api_data = self.route_query(query, query_time)
        
        # 检索相关文档
        retrieved_docs = self.retriever.retrieve(query, search_results)
        
        # 合并检索结果和API结果
        all_context = retrieved_docs
        if api_data:
            all_context.extend([f"Real-time data: {item}" for item in api_data])
        
        # 构造prompt
        context_text = "\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(all_context)])
        
        prompt = f"""
        You are a helpful assistant answering questions based on provided context.
        
        Domain: {domain}
        Dynamic Level: {dynamic_level}
        Query: {query}
        
        Context:
        {context_text}
        
        Please provide a comprehensive answer based on the context above. 
        If the context doesn't contain relevant information, clearly state that.
        Cite specific sources when possible.
        
        Answer:
        """
        
        # 调用生成模型
        response = self.chat_model.generate(prompt)
        return response
    
    def batch_generate_answer(self, batch):
        """批量生成答案"""
        answers = []
        
        for i in range(len(batch["query"])):
            query = batch["query"][i]
            search_results = batch["search_results"][i]
            query_time = batch["query_time"][i]
            
            try:
                answer = self.generate_answer(query, search_results, query_time)
                answers.append(answer)
            except Exception as e:
                print(f"Error generating answer for query {i}: {e}")
                answers.append("Sorry, I couldn't generate an answer for this question.")
        
        return answers
```

### 4.2 完整推理脚本
创建 `src/pipeline_infer.py`：

```python
import os
import sys
sys.path.append('src')

from models.load_model import load_model
from models.router.router import SequenceClassificationRouter  
from models.retrieve.retriever import Retriever
from models.model import RAGModel
import json
import bz2
from tqdm import tqdm

def main():
    # 环境配置
    os.environ["CRAG_MOCK_API_URL"] = "http://localhost:8000"
    
    # 1. 加载生成模型
    chat_model = load_model(
        model_name="gpt-4",
        api_key="your-api-key",
        base_url="https://api.openai.com/v1",
        temperature=0
    )
    
    # 2. 加载检索器
    retriever = Retriever(
        top_k_retrieve=10,
        top_k_rerank=5, 
        embedding_model_path="models/retrieve/embedding_models/bge-m3",
        reranker_model_path="models/retrieve/reranker_models/bge-reranker-v2-m3",
        rerank=True
    )
    
    # 3. 加载路由器
    domain_router = SequenceClassificationRouter(
        model_path="models/router/domain",
        classes=["finance", "music", "movie", "sports", "open"]
    )
    
    dynamic_router = SequenceClassificationRouter(
        model_path="models/router/dynamic", 
        classes=["static", "slow-changing", "fast-changing", "real-time"]
    )
    
    # 4. 构建RAG模型
    rag_model = RAGModel(
        chat_model=chat_model,
        retriever=retriever,
        domain_router=domain_router,
        dynamic_router=dynamic_router,
        use_kg=True
    )
    
    # 5. 执行推理
    dataset_path = "data/cleaned/dev_data.jsonl.bz2"
    queries, ground_truths, predictions = generate_predictions(dataset_path, rag_model)
    
    # 6. 保存结果
    output_path = "results/gpt4_predictions.jsonl"
    os.makedirs("results", exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as file:
        for query, ground_truth, prediction in zip(queries, ground_truths, predictions):
            item = {
                "query": query,
                "ground_truth": ground_truth, 
                "prediction": prediction
            }
            file.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"Results saved to {output_path}")

if __name__ == "__main__":
    main()
```

---

## 阶段五：评估与改进（第5-6周）

### 5.1 实现评估脚本
创建 `src/eval.py`：

```python
import json
import re
from collections import Counter
import numpy as np

def normalize_answer(text):
    """标准化答案文本"""
    # 转小写，移除标点和多余空格
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def exact_match_score(prediction, ground_truth):
    """计算精确匹配分数"""
    return normalize_answer(prediction) == normalize_answer(ground_truth)

def f1_score(prediction, ground_truth):
    """计算F1分数"""
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()
    
    if len(pred_tokens) == 0 and len(truth_tokens) == 0:
        return 1.0
    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return 0.0
    
    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)
    num_common = sum(common_tokens.values())
    
    precision = num_common / len(pred_tokens)
    recall = num_common / len(truth_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    f1 = 2 * precision * recall / (precision + recall)
    return f1

def evaluate_predictions(predictions_file):
    """评估预测结果"""
    exact_matches = []
    f1_scores = []
    
    with open(predictions_file, 'r', encoding='utf-8') as file:
        for line in file:
            item = json.loads(line)
            prediction = item["prediction"]
            ground_truth = item["ground_truth"]
            
            em = exact_match_score(prediction, ground_truth)
            f1 = f1_score(prediction, ground_truth)
            
            exact_matches.append(em)
            f1_scores.append(f1)
    
    # 计算平均分数
    avg_em = np.mean(exact_matches)
    avg_f1 = np.mean(f1_scores)
    
    print(f"Exact Match: {avg_em:.4f}")
    print(f"F1 Score: {avg_f1:.4f}")
    print(f"Total samples: {len(exact_matches)}")
    
    return avg_em, avg_f1

if __name__ == "__main__":
    evaluate_predictions("results/gpt4_predictions.jsonl")
```

### 5.2 改进方向
根据知识库建议，重点关注以下改进：

1. **更强的Reranker**：
   - 使用cross-encoder模型精排
   - 尝试多阶段检索

2. **更细粒度的Router**：
   - 增加子领域分类（如"科技金融"、"流行音乐"）
   - 时间敏感性更精细分类

3. **Query Preprocessing优化**：
   - 查询扩展和改写
   - 实体识别和链接

---

## 部署与运行

### 项目目录结构
```
crag_reproduction/
├── src/
│   ├── preprocessing.py          # 数据清洗
│   ├── indexer.py               # 索引构建  
│   ├── pipeline_infer.py        # 端到端推理
│   ├── eval.py                  # 评估脚本
│   ├── models/
│   │   ├── load_model.py        # 模型加载
│   │   ├── model.py             # RAG主模型
│   │   └── retrieve/
│   │       └── retriever.py     # 检索器
│   └── routers/
│       ├── router.py            # 路由器基类
│       ├── train_domain_router.py
│       └── train_dynamic_router.py
├── data/
│   ├── raw/                     # 原始数据
│   └── cleaned/                 # 清洗后数据
├── models/                      # 模型文件
├── index/                       # 索引文件
├── results/                     # 结果输出
└── requirements.txt
```

### 快速启动脚本
创建 `run_pipeline.sh`：

```bash
#!/bin/bash

# 1. 数据预处理
echo "Step 1: Data preprocessing..."
python src/preprocessing.py --input data/raw/crag_dev.jsonl --output data/cleaned/dev_data.jsonl.bz2

# 2. 构建索引
echo "Step 2: Building index..."  
python src/indexer.py --data data/cleaned/dev_data.jsonl --output index/

# 3. 训练路由器
echo "Step 3: Training routers..."
python src/routers/train_domain_router.py --data data/cleaned/dev_data.jsonl --output models/router/domain
python src/routers/train_dynamic_router.py --data data/cleaned/dev_data.jsonl --output models/router/dynamic

# 4. 端到端推理
echo "Step 4: Running inference..."
python src/pipeline_infer.py

# 5. 评估结果
echo "Step 5: Evaluation..."
python src/eval.py --predictions results/gpt4_predictions.jsonl

echo "Pipeline completed!"
```

这个详细计划涵盖了从数据预处理到模型训练、推理和评估的完整流程，严格对应Meta CRAG比赛要求和USTC第二名方案的技术路线。你可以按照这个计划逐步实现，每个阶段都有具体的代码实现和评估标准。
